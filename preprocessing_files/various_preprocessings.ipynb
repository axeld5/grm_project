{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports \n",
    "\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_add_pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained word embedding model\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie is another Christian propaganda fil...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A woman who hates cats (Alice Krige) and her s...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beast Wars is a show that is over-hyped, overp...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An excellent example of \"cowboy noir\", as it's...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ok, basically this is a popcorn sci-fi movie, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  This movie is another Christian propaganda fil...    0.0\n",
       "1  A woman who hates cats (Alice Krige) and her s...    1.0\n",
       "2  Beast Wars is a show that is over-hyped, overp...    0.0\n",
       "3  An excellent example of \"cowboy noir\", as it's...    1.0\n",
       "4  Ok, basically this is a popcorn sci-fi movie, ...    1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "df = df.sample(frac=1, random_state = 42).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 2 : Les biagrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biagram_preprocessing_of_text(review, label): \n",
    "\n",
    "    ## sentences preprocessing\n",
    "    doc = nlp(review)\n",
    "    sentences = [sent for sent in doc.sents]\n",
    "    sentences = [[token.text.lower() for token in sent if not token.is_stop and token.is_alpha] for sent in sentences]\n",
    "    sentences = [[word for word in sent if len(word) > 1] for sent in sentences]\n",
    "\n",
    "    ## get the biagrams\n",
    "    biagrams = []\n",
    "    for sent in sentences:\n",
    "        for i in range(len(sent)-1):\n",
    "            biagrams.append((sent[i], sent[i+1]))\n",
    "    \n",
    "    ### dico of how many times a biagram appears in the review\n",
    "    dico_biagrams = {}\n",
    "    for biagram in biagrams:\n",
    "        if biagram not in dico_biagrams and (biagram[1], biagram[0]) not in dico_biagrams:\n",
    "            dico_biagrams[biagram] = 1\n",
    "        elif biagram in dico_biagrams:\n",
    "            dico_biagrams[biagram] += 1\n",
    "        elif (biagram[1], biagram[0]) in dico_biagrams:\n",
    "            dico_biagrams[(biagram[1], biagram[0])] += 1\n",
    "        \n",
    "\n",
    "    list_of_words = [word for sent in sentences for word in sent]\n",
    "    list_of_words = list(set(list_of_words))\n",
    "    ## create graph \n",
    "    G = nx.Graph()\n",
    "    ## nodes as words \n",
    "    G.add_nodes_from(list_of_words)\n",
    "\n",
    "    ## add edges\n",
    "    for biagram in dico_biagrams.keys():\n",
    "        G.add_edge(biagram[0], biagram[1], weight = dico_biagrams[biagram])\n",
    "\n",
    "    \n",
    "    # Get the node features\n",
    "    node_features = []\n",
    "    for node in G.nodes():\n",
    "        node_features.append(nlp.vocab[node].vector)\n",
    "    node_features = np.array(node_features)\n",
    "    # Get the edges\n",
    "    edges = []\n",
    "    for edge in G.edges():\n",
    "        edges.append([list(G.nodes()).index(edge[0]), list(G.nodes()).index(edge[1])])\n",
    "    edges = np.array(edges)\n",
    "    ## get edges \n",
    "    edges_attr  = []\n",
    "    for edge in G.edges():\n",
    "        edges_attr.append([G.edges[edge]['weight']])\n",
    "    edges_attr = np.array(edges_attr)\n",
    "    \n",
    "    # Get the label\n",
    "    label_value = int(label)\n",
    "\n",
    "\n",
    "    # Create a PyTorch Geometric Data object\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edges.T, dtype=torch.long)\n",
    "    y = torch.tensor(label_value, dtype=torch.float)\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr = torch.tensor(edges_attr, dtype=torch.float), y=y) ### here carefull, by adding the weights, the results are worse ! \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention graphs ( does not work, takes forever to run, do not use !)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had done previous tests by getting the attention weights from a bert and not one layer and it took more time so I abandonned that and this is my attempt at attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads=1):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(input_dim, num_heads)\n",
    "        self.linear = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Permute the tensor to have shape (sentence_length, batch_size, input_dim)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        # Apply linear transformation to get query, key, and value tensors\n",
    "        q = self.linear(x)\n",
    "        k = self.linear(x)\n",
    "        v = self.linear(x)\n",
    "        # Apply multi-head attention\n",
    "        attn_output, _ = self.attention(q, k, v)\n",
    "        # Reshape the output and apply dropout\n",
    "        attn_output = attn_output.permute(1, 0, 2)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        # Apply linear transformation to get hidden representations\n",
    "        hidden = self.linear(attn_output)\n",
    "        # Apply dropout and return the output\n",
    "        output = self.dropout(hidden)\n",
    "        return output\n",
    "\n",
    "\n",
    "def attention_preprocessing(review, label) :\n",
    "    embedding_dim = 768\n",
    "    hidden_dim = 768   \n",
    "    input_dim = embedding_dim\n",
    "\n",
    "\n",
    "    doc = nlp(review)\n",
    "    sentences = [sent for sent in doc.sents]\n",
    "    sentences = [[token.text.lower() for token in sent if not token.is_stop and token.is_alpha] for sent in sentences]\n",
    "    sentences = [[word for word in sent if len(word) > 1] for sent in sentences]\n",
    "\n",
    "    G = nx.Graph()\n",
    "    list_of_words = [word for sent in sentences for word in sent]\n",
    "    list_of_words = list(set(list_of_words))\n",
    "\n",
    "    ## nodes as words \n",
    "    G.add_nodes_from(list_of_words)\n",
    "    ## add edges \n",
    "    for sent in sentences:\n",
    "        tokens = sent \n",
    "        input_tensor = torch.randn(1, len(tokens), input_dim)\n",
    "        embeddings = input_tensor.transpose(0, 1)\n",
    "        output = AttentionLayer(input_dim, hidden_dim, num_heads=1)(embeddings)\n",
    "        output = output.transpose(0, 1)\n",
    "        output = torch.mean(output, dim=2)\n",
    "\n",
    "        ## loop in tokens and add edges with the attention weights\n",
    "        for i in range(len(tokens)):\n",
    "            for j in range(len(tokens)):\n",
    "                if i != j:\n",
    "                    if (tokens[i], tokens[j]) not in G.edges() and (tokens[j], tokens[i]) not in G.edges():\n",
    "                        if output[0][i].item() > 0:\n",
    "                            G.add_edge(tokens[i], tokens[j], weight=output[0][i].item())\n",
    "\n",
    "    \n",
    "    \n",
    "    # Get the node features\n",
    "    node_features = []\n",
    "    for node in G.nodes():\n",
    "        node_features.append(nlp.vocab[node].vector)\n",
    "    node_features = np.array(node_features)\n",
    "    # Get the edges\n",
    "    edges = []\n",
    "    for edge in G.edges():\n",
    "        edges.append([list(G.nodes()).index(edge[0]), list(G.nodes()).index(edge[1])])\n",
    "    edges = np.array(edges)\n",
    "\n",
    "    ## get edge_attr\n",
    "    edges_attr  = []\n",
    "    for edge in G.edges():\n",
    "        edges_attr.append([G.edges[edge]['weight']])\n",
    "    edges_attr = np.array(edges_attr)\n",
    "\n",
    "\n",
    "    # Get the label\n",
    "    label_value = int(label)\n",
    "\n",
    "    # Create a PyTorch Geometric Data object\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edges.T, dtype=torch.long)\n",
    "    y = torch.tensor(label_value, dtype=torch.float)\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr = torch.tensor(edges_attr, dtype=torch.float), y=y)\n",
    "\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work in progress, create graphs depending on adjectifs... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to join bigrams and this method to create graphs with bigrams + wights on what type of connection it is, and not number of time that connection is present. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adjectif_preprocessing(review, label, look_for = 'ADJ') : \n",
    "    doc = nlp(review) \n",
    "\n",
    "\n",
    "    sentences = [sent for sent in doc.sents]\n",
    "    sentences = [{token.text.lower() : (token.pos_, token.dep_) for token in sent if not token.is_stop and token.is_alpha} for sent in sentences]\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    ## get the keys of the dictionary\n",
    "    list_of_words = [word for sent in sentences for word in sent.keys()]\n",
    "    ## get the unique words\n",
    "    list_of_adverbs = []\n",
    "    for sent in sentences:\n",
    "        for word in sent:\n",
    "            if sent[word][0] == look_for:\n",
    "                list_of_adverbs.append(word)\n",
    "\n",
    "\n",
    "    \n",
    "    list_of_word_set = list(set(list_of_adverbs))\n",
    "\n",
    "    ## nodes as words \n",
    "    G.add_nodes_from(list_of_word_set)\n",
    "    ## add edges \n",
    "\n",
    "    ## create edges between each word in list_of_word\n",
    "    for word in list_of_word_set:\n",
    "        for word2 in list_of_word_set:\n",
    "            if word != word2:\n",
    "                G.add_edge(word, word2, weight=1)\n",
    "    \n",
    "\n",
    "\n",
    "        # Get the node features, the node feature are the word embeddings\n",
    "    node_features = []\n",
    "    for node in G.nodes():\n",
    "        node_features.append(nlp.vocab[node].vector)\n",
    "    node_features = np.array(node_features)\n",
    "\n",
    "    # Get the edges\n",
    "    edges = []\n",
    "    for edge in G.edges():\n",
    "        edges.append([list(G.nodes()).index(edge[0]), list(G.nodes()).index(edge[1])])\n",
    "    edges = np.array(edges)\n",
    "\n",
    "    ## get edge_attr\n",
    "    edges_attr  = []\n",
    "    for edge in G.edges():\n",
    "        edges_attr.append([G.edges[edge]['weight']])\n",
    "    edges_attr = np.array(edges_attr)\n",
    "\n",
    "    \n",
    "    # Get the label\n",
    "    label_value = int(label)\n",
    "\n",
    "    # Create a PyTorch Geometric Data object\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edges.T, dtype=torch.long)\n",
    "    y = torch.tensor(label_value, dtype=torch.float)\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr= torch.tensor(edges_attr, dtype=torch.float), y=y)\n",
    "\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good preprocessing !! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biagram_depending_on_link(review, label):\n",
    "\n",
    "    weights_each_type = {'ADJ': 3, 'ADV': 2, 'NOUN': 1, 'VERB': 4, 'ADP': 1, 'DET': 1, 'NUM': 1, 'PUNCT': 1, 'PRON': 1, 'PROPN': 1, 'SCONJ': 1, 'SYM': 1, 'X': 1, 'PART': 1, 'CCONJ': 1, 'INTJ': 1, 'AUX': 1, 'SPACE': 1, '': 1}\n",
    "\n",
    "    ## sentences preprocessing\n",
    "    doc = nlp(review)\n",
    "    sentences = [sent for sent in doc.sents]\n",
    "    sentences = [{token.text.lower() : (token.pos_, token.dep_) for token in sent if not token.is_stop and token.is_alpha} for sent in sentences]\n",
    "\n",
    "    ## get the biagrams\n",
    "    biagrams = []\n",
    "    for sent in sentences:\n",
    "        for i in range(len(sent)-1):\n",
    "            biagrams.append((list(sent.keys())[i], list(sent.keys())[i+1]))\n",
    "\n",
    "    \n",
    "    ### concatenate all the sentences in one\n",
    "    sent = {}\n",
    "    for sentence in sentences:\n",
    "        sent.update(sentence)\n",
    "\n",
    "    ### dico of how many times a biagram appears in the review\n",
    "    dico_biagrams = {}\n",
    "    for biagram in biagrams:\n",
    "        if biagram not in dico_biagrams and (biagram[1], biagram[0]) not in dico_biagrams:\n",
    "            dico_biagrams[biagram] = weights_each_type[sent[biagram[0]][0]] + weights_each_type[sent[biagram[1]][0]]\n",
    "        elif biagram in dico_biagrams:\n",
    "            dico_biagrams[biagram] +=  weights_each_type[sent[biagram[0]][0]] + weights_each_type[sent[biagram[1]][0]]\n",
    "        elif (biagram[1], biagram[0]) in dico_biagrams:\n",
    "            dico_biagrams[(biagram[1], biagram[0])] += weights_each_type[sent[biagram[0]][0]] + weights_each_type[sent[biagram[1]][0]]\n",
    "        \n",
    "\n",
    "    list_of_words = [word for sent in sentences for word in sent]\n",
    "    list_of_words = list(set(list_of_words))\n",
    "    ## create graph \n",
    "    G = nx.Graph()\n",
    "    ## nodes as words \n",
    "    G.add_nodes_from(list_of_words)\n",
    "\n",
    "    ## add edges\n",
    "    for biagram in dico_biagrams.keys():\n",
    "        G.add_edge(biagram[0], biagram[1], weight = dico_biagrams[biagram])\n",
    "\n",
    "    \n",
    "    # Get the node features\n",
    "    node_features = []\n",
    "    for node in G.nodes():\n",
    "        node_features.append(nlp.vocab[node].vector)\n",
    "    node_features = np.array(node_features)\n",
    "    # Get the edges\n",
    "    edges = []\n",
    "    for edge in G.edges():\n",
    "        edges.append([list(G.nodes()).index(edge[0]), list(G.nodes()).index(edge[1])])\n",
    "    edges = np.array(edges)\n",
    "    ## edge_attr \n",
    "    edges_attr  = []\n",
    "    for edge in G.edges():\n",
    "        edges_attr.append([G.edges[edge]['weight']])\n",
    "    edges_attr = np.array(edges_attr)\n",
    "    \n",
    "    # Get the label\n",
    "    label_value = int(label)\n",
    "\n",
    "\n",
    "    # Create a PyTorch Geometric Data object\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edges.T, dtype=torch.long)\n",
    "    y = torch.tensor(label_value, dtype=torch.float)\n",
    "    edge_attr = torch.tensor(edges_attr, dtype=torch.float)\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr = edge_attr, y=y)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test on 10 reviews \n",
    "list_of_reviews = [] \n",
    "\n",
    "for i in range(10):\n",
    "    data = biagram_preprocessing_of_text(df['review'][i], df['label'][i])\n",
    "    list_of_reviews.append(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[57, 300], edge_index=[2, 58], edge_attr=[58, 1], y=0.0),\n",
       " Data(x=[49, 300], edge_index=[2, 44], edge_attr=[44, 1], y=1.0),\n",
       " Data(x=[54, 300], edge_index=[2, 47], edge_attr=[47, 1], y=0.0),\n",
       " Data(x=[173, 300], edge_index=[2, 186], edge_attr=[186, 1], y=1.0),\n",
       " Data(x=[40, 300], edge_index=[2, 36], edge_attr=[36, 1], y=1.0),\n",
       " Data(x=[75, 300], edge_index=[2, 70], edge_attr=[70, 1], y=0.0),\n",
       " Data(x=[38, 300], edge_index=[2, 32], edge_attr=[32, 1], y=0.0),\n",
       " Data(x=[33, 300], edge_index=[2, 28], edge_attr=[28, 1], y=1.0),\n",
       " Data(x=[50, 300], edge_index=[2, 47], edge_attr=[47, 1], y=0.0),\n",
       " Data(x=[43, 300], edge_index=[2, 41], edge_attr=[41, 1], y=0.0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test on 10 reviews \n",
    "list_of_reviews = [] \n",
    "\n",
    "for i in range(10):\n",
    "    data = adjectif_preprocessing(df['review'][i], df['label'][i])\n",
    "    list_of_reviews.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[6, 300], edge_index=[2, 15], edge_attr=[15, 1], y=0.0),\n",
       " Data(x=[8, 300], edge_index=[2, 28], edge_attr=[28, 1], y=1.0),\n",
       " Data(x=[5, 300], edge_index=[2, 10], edge_attr=[10, 1], y=0.0),\n",
       " Data(x=[38, 300], edge_index=[2, 703], edge_attr=[703, 1], y=1.0),\n",
       " Data(x=[6, 300], edge_index=[2, 15], edge_attr=[15, 1], y=1.0),\n",
       " Data(x=[15, 300], edge_index=[2, 105], edge_attr=[105, 1], y=0.0),\n",
       " Data(x=[7, 300], edge_index=[2, 21], edge_attr=[21, 1], y=0.0),\n",
       " Data(x=[6, 300], edge_index=[2, 15], edge_attr=[15, 1], y=1.0),\n",
       " Data(x=[11, 300], edge_index=[2, 55], edge_attr=[55, 1], y=0.0),\n",
       " Data(x=[6, 300], edge_index=[2, 15], edge_attr=[15, 1], y=0.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[57, 300], edge_index=[2, 205], edge_attr=[205, 1], y=0.0),\n",
       " Data(x=[49, 300], edge_index=[2, 352], edge_attr=[352, 1], y=1.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test on 2 reviews \n",
    "list_of_reviews = [] \n",
    "\n",
    "for i in range(2):\n",
    "    data = attention_preprocessing(df['review'][i], df['label'][i])\n",
    "    list_of_reviews.append(data)\n",
    "\n",
    "list_of_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[57, 300], edge_index=[2, 55], edge_attr=[55, 1], y=0.0),\n",
       " Data(x=[49, 300], edge_index=[2, 44], edge_attr=[44, 1], y=1.0),\n",
       " Data(x=[54, 300], edge_index=[2, 47], edge_attr=[47, 1], y=0.0),\n",
       " Data(x=[173, 300], edge_index=[2, 185], edge_attr=[185, 1], y=1.0),\n",
       " Data(x=[40, 300], edge_index=[2, 36], edge_attr=[36, 1], y=1.0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_reviews = [] \n",
    "\n",
    "for i in range(5):\n",
    "    data = biagram_depending_on_link(df['review'][i], df['label'][i])\n",
    "    list_of_reviews.append(data)\n",
    "\n",
    "list_of_reviews"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
