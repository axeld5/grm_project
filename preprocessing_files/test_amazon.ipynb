{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_add_pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to read the files \n",
    "\n",
    "def read_file(path, number):\n",
    "    whole_data = []\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        for i in range(number):\n",
    "            line = f.readline()\n",
    "            data = json.loads(line)\n",
    "            if 'reviewText' in data:\n",
    "                value = data['reviewText'].replace('\\n', ' ')\n",
    "                whole_data.append(value)\n",
    "    \n",
    "    return whole_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function that writes the csv file\n",
    "\n",
    "def write_csv_file(path, data, number, label):\n",
    "    data = [x for x in data if len(x.split()) > 10]\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['reviewText', 'label'])\n",
    "        for i in range(number):\n",
    "            writer.writerow([data[i], label])\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion, label 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'amazon_data/AMAZON_FASHION_5.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\axeld\\Desktop\\coding\\grm_project\\preprocessing_files\\test_amazon.ipynb Cell 7\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/axeld/Desktop/coding/grm_project/preprocessing_files/test_amazon.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mamazon_data/AMAZON_FASHION_5.json\u001b[39m\u001b[39m'\u001b[39m \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/axeld/Desktop/coding/grm_project/preprocessing_files/test_amazon.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m fashion_data \u001b[39m=\u001b[39m read_file(path, \u001b[39m2000\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/axeld/Desktop/coding/grm_project/preprocessing_files/test_amazon.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m write_csv_file(\u001b[39m'\u001b[39m\u001b[39mmulti_class_data/fashion.csv\u001b[39m\u001b[39m'\u001b[39m, fashion_data, \u001b[39m1000\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\axeld\\Desktop\\coding\\grm_project\\preprocessing_files\\test_amazon.ipynb Cell 7\u001b[0m in \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/axeld/Desktop/coding/grm_project/preprocessing_files/test_amazon.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_file\u001b[39m(path, number):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/axeld/Desktop/coding/grm_project/preprocessing_files/test_amazon.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     whole_data \u001b[39m=\u001b[39m []\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/axeld/Desktop/coding/grm_project/preprocessing_files/test_amazon.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(path, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/axeld/Desktop/coding/grm_project/preprocessing_files/test_amazon.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(number):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/axeld/Desktop/coding/grm_project/preprocessing_files/test_amazon.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m             line \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreadline()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'amazon_data/AMAZON_FASHION_5.json'"
     ]
    }
   ],
   "source": [
    "path = 'amazon_data/AMAZON_FASHION_5.json' \n",
    "fashion_data = read_file(path, 2000)\n",
    "write_csv_file('multi_class_data/fashion.csv', fashion_data, 1000, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Music, label 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'amazon_Data/Digital_Music_5.json'\n",
    "music_data = read_file(path, 2000)\n",
    "write_csv_file('multi_class_data/music.csv', music_data, 1000, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sports, label 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'amazon_data/Sports_and_Outdoors_5.json'\n",
    "sport_data = read_file(path, 2000)\n",
    "write_csv_file('multi_class_data/sport.csv', sport_data, 1000, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pet Supplies, label 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'amazon_data/Pet_Supplies_5.json'\n",
    "pet_data = read_file(path, 2000)\n",
    "write_csv_file('multi_class_data/pet.csv', pet_data, 1000, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fashion = pd.read_csv('multi_class_data/fashion.csv')\n",
    "df_music = pd.read_csv('multi_class_data/music.csv')\n",
    "df_sport = pd.read_csv('multi_class_data/sport.csv')\n",
    "df_pet = pd.read_csv('multi_class_data/pet.csv')\n",
    "\n",
    "df = pd.concat([df_fashion, df_music, df_sport, df_pet], ignore_index=True)\n",
    "df = df.sample(frac=1, random_state = 42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the best sneakers by far! I had never owned a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The quality of the items were good and they we...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>These shoes are extremely comfortable, and fit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They worked extremely well, this is the only p...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Convenient packaging and reasonable pricing. N...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  label\n",
       "0  the best sneakers by far! I had never owned a ...      0\n",
       "1  The quality of the items were good and they we...      3\n",
       "2  These shoes are extremely comfortable, and fit...      0\n",
       "3  They worked extremely well, this is the only p...      3\n",
       "4  Convenient packaging and reasonable pricing. N...      2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained word embedding model\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biagram_depending_on_link(review, label):\n",
    "    \n",
    "    weights_each_type = {'ADJ': 3, 'ADV': 2, 'NOUN': 1, 'VERB': 4, 'ADP': 1, 'DET': 1, 'NUM': 1, 'PUNCT': 1, 'PRON': 1, 'PROPN': 1, 'SCONJ': 1, 'SYM': 1, 'X': 1, 'PART': 1, 'CCONJ': 1, 'INTJ': 1, 'AUX': 1, 'SPACE': 1, '': 1}\n",
    "\n",
    "    ## sentences preprocessing\n",
    "    doc = nlp(review)\n",
    "    sentences = [sent for sent in doc.sents]\n",
    "    sentences = [{token.text.lower() : (token.pos_, token.dep_) for token in sent if not token.is_stop and token.is_alpha} for sent in sentences]\n",
    "\n",
    "    ## get the biagrams\n",
    "    biagrams = []\n",
    "    for sent in sentences:\n",
    "        for i in range(len(sent)-1):\n",
    "            biagrams.append((list(sent.keys())[i], list(sent.keys())[i+1]))\n",
    "\n",
    "    \n",
    "    ### concatenate all the sentences in one\n",
    "    sent = {}\n",
    "    for sentence in sentences:\n",
    "        sent.update(sentence)\n",
    "\n",
    "    ### dico of how many times a biagram appears in the review\n",
    "    dico_biagrams = {}\n",
    "    for biagram in biagrams:\n",
    "        if biagram not in dico_biagrams and (biagram[1], biagram[0]) not in dico_biagrams:\n",
    "            dico_biagrams[biagram] = weights_each_type[sent[biagram[0]][0]] + weights_each_type[sent[biagram[1]][0]]\n",
    "        elif biagram in dico_biagrams:\n",
    "            dico_biagrams[biagram] +=  weights_each_type[sent[biagram[0]][0]] + weights_each_type[sent[biagram[1]][0]]\n",
    "        elif (biagram[1], biagram[0]) in dico_biagrams:\n",
    "            dico_biagrams[(biagram[1], biagram[0])] += weights_each_type[sent[biagram[0]][0]] + weights_each_type[sent[biagram[1]][0]]\n",
    "        \n",
    "\n",
    "    list_of_words = [word for sent in sentences for word in sent]\n",
    "    list_of_words = list(set(list_of_words))\n",
    "    ## create graph \n",
    "    G = nx.Graph()\n",
    "    ## nodes as words \n",
    "    G.add_nodes_from(list_of_words)\n",
    "\n",
    "    ## add edges\n",
    "    for biagram in dico_biagrams.keys():\n",
    "        G.add_edge(biagram[0], biagram[1], weight = dico_biagrams[biagram])\n",
    "\n",
    "    \n",
    "    # Get the node features\n",
    "    node_features = []\n",
    "    for node in G.nodes():\n",
    "        node_features.append(nlp.vocab[node].vector)\n",
    "    node_features = np.array(node_features)\n",
    "    # Get the edges\n",
    "    edges = []\n",
    "    for edge in G.edges():\n",
    "        edges.append([list(G.nodes()).index(edge[0]), list(G.nodes()).index(edge[1])])\n",
    "    edges = np.array(edges)\n",
    "    ## edge_attr \n",
    "    edges_attr  = []\n",
    "    for edge in G.edges():\n",
    "        edges_attr.append([G.edges[edge]['weight']])\n",
    "    edges_attr = np.array(edges_attr)\n",
    "    \n",
    "    # Get the label\n",
    "    label_value = int(label)\n",
    "\n",
    "\n",
    "    # Create a PyTorch Geometric Data object\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edges.T, dtype=torch.long)\n",
    "    y = torch.tensor(label_value, dtype=torch.float)\n",
    "    edge_attr = torch.tensor(edges_attr, dtype=torch.float)\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr = edge_attr, y=y)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_reviews = [] \n",
    "\n",
    "for i in range(800):\n",
    "    data = biagram_depending_on_link(df['reviewText'][i], df['label'][i])\n",
    "    list_of_reviews.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[11, 300], edge_index=[2, 9], edge_attr=[9, 1], y=0.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(list_of_reviews, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "263\n",
      "tensor(3.)\n",
      "torch.Size([7, 300])\n",
      "639\n"
     ]
    }
   ],
   "source": [
    "for i,review in enumerate(train_data):\n",
    "    if review.edge_index.shape < torch.Size([2]):\n",
    "        print(review.edge_index.shape)\n",
    "        print(i)\n",
    "        print(review.y)\n",
    "        print(review.x.shape)\n",
    "        train_data.pop(i)\n",
    "print(len(train_data))\n",
    "\n",
    "for i,review in enumerate(test_data):\n",
    "    if review.edge_index.shape < torch.Size([2]):\n",
    "        print(review.edge_index.shape)\n",
    "        test_data.pop(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### is the data balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fashion reviews: 160\n",
      "Number of music reviews: 175\n",
      "Number of sport reviews: 160\n",
      "Number of pet reviews: 144\n"
     ]
    }
   ],
   "source": [
    "def get_numbers(data):\n",
    "    labels = [d.y.item() for d in data]\n",
    "    fashion = labels.count(0)   \n",
    "    music = labels.count(1)\n",
    "    sport = labels.count(2)\n",
    "    pet = labels.count(3)\n",
    "\n",
    "    return fashion, music, sport, pet\n",
    "\n",
    "fashion, music, sport, pet = get_numbers(train_data)\n",
    "\n",
    "print(f'Number of fashion reviews: {fashion}')\n",
    "print(f'Number of music reviews: {music}')\n",
    "print(f'Number of sport reviews: {sport}')\n",
    "print(f'Number of pet reviews: {pet}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\axeld\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 1.1256, Train Acc: 0.6432 Test Acc: 0.6750\n",
      "Epoch: 001, Loss: 0.6655, Train Acc: 0.8529 Test Acc: 0.8500\n",
      "Epoch: 002, Loss: 0.3627, Train Acc: 0.8936 Test Acc: 0.8438\n",
      "Epoch: 003, Loss: 0.2644, Train Acc: 0.9562 Test Acc: 0.9000\n",
      "Epoch: 004, Loss: 0.2026, Train Acc: 0.9734 Test Acc: 0.9000\n",
      "Epoch: 005, Loss: 0.1279, Train Acc: 0.9797 Test Acc: 0.9000\n",
      "Epoch: 006, Loss: 0.1071, Train Acc: 0.9890 Test Acc: 0.9062\n",
      "Epoch: 007, Loss: 0.0817, Train Acc: 0.9828 Test Acc: 0.9062\n",
      "Epoch: 008, Loss: 0.0354, Train Acc: 0.9984 Test Acc: 0.9187\n",
      "Epoch: 009, Loss: 0.0728, Train Acc: 0.9906 Test Acc: 0.9000\n",
      "\n",
      "GraphSage test accuracy: 90.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, global_add_pool, global_mean_pool  \n",
    "\n",
    "'''\n",
    "Graph SAGE: SAmpling and aggreGatE, \n",
    "Samples only a subset of neighboring nodes at different depth layers, \n",
    "and then the aggregator takes neighbors of the previous layers and aggregates them\n",
    "'''\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "  \"\"\"GraphSAGE\"\"\"\n",
    "  def __init__(self, num_node_features, hidden_dim, num_classes):\n",
    "    super().__init__()\n",
    "    self.sage1 = SAGEConv(num_node_features, hidden_dim*2)\n",
    "    self.sage2 = SAGEConv(hidden_dim*2, hidden_dim)\n",
    "    self.sage3 = SAGEConv(hidden_dim, hidden_dim)\n",
    "    self.sage4 = SAGEConv(hidden_dim, num_classes)\n",
    "    self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                      lr=0.0001,\n",
    "                                        weight_decay=5e-4)\n",
    "                                      \n",
    "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    ## layer 1 \n",
    "    h = self.sage1(x, edge_index)\n",
    "    h = torch.relu(h)\n",
    "    h = F.dropout(h, p=0.2, training=self.training)\n",
    "\n",
    "    ## layer 2\n",
    "\n",
    "    h = self.sage2(h, edge_index)\n",
    "    h = torch.relu(h)\n",
    "    h = F.dropout(h, p=0.2, training=self.training)\n",
    "\n",
    "    # layer 3 \n",
    "    h = self.sage3(h, edge_index)\n",
    "    h = torch.relu(h)\n",
    "    h = F.dropout(h, p=0.5, training=self.training)\n",
    "\n",
    "     # layer 4\n",
    "    h = self.sage3(h, edge_index)\n",
    "    h = torch.relu(h)\n",
    "    h = F.dropout(h, p=0.5, training=self.training)\n",
    "\n",
    "     # layer 5\n",
    "    h = self.sage3(h, edge_index)\n",
    "    h = torch.relu(h)\n",
    "    h = F.dropout(h, p=0.2, training=self.training)\n",
    "\n",
    "    ## layer 6\n",
    "    h = self.sage4(h, edge_index)\n",
    "    h = global_mean_pool(h, torch.zeros(h.size(0), dtype=torch.long).to(self.device))\n",
    "    return h, F.log_softmax(h, dim=1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "def train(model, loader, device):\n",
    "    model.train()\n",
    "    optimizer = model.optimizer\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        _, out = model(data.x.float(), data.edge_index)\n",
    "        loss = F.nll_loss(out, data.y.long())\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return total_loss / len(loader.dataset)\n",
    "    \n",
    "    \n",
    "# Define the testing loop\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        _, out = model(data.x.float(), data.edge_index)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += pred.eq(data.y.long()).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "\n",
    "# Define the model and optimizer\n",
    "model = GraphSAGE(300, 300, 4).to(device)\n",
    "\n",
    "# Train the model\n",
    "train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "for epoch in range(0, 10):\n",
    "    loss = train(model, train_loader, device)\n",
    "    train_acc = test(model, train_loader, device)\n",
    "    test_acc = test(model, test_loader, device)\n",
    "    print(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}\", f\"Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "\n",
    "print(f'\\nGraphSage test accuracy: {test(model, test_loader, device)*100:.2f}%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
