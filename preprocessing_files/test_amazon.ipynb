{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_add_pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to read the files \n",
    "\n",
    "def read_file(path, number):\n",
    "    whole_data = []\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        for i in range(number):\n",
    "            line = f.readline()\n",
    "            data = json.loads(line)\n",
    "            if 'reviewText' in data:\n",
    "                value = data['reviewText'].replace('\\n', ' ')\n",
    "                whole_data.append(value)\n",
    "    \n",
    "    return whole_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function that writes the csv file\n",
    "\n",
    "def write_csv_file(path, data, number, label):\n",
    "    data = [x for x in data if len(x.split()) > 10]\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['reviewText', 'label'])\n",
    "        for i in range(number):\n",
    "            writer.writerow([data[i], label])\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion, label 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'amazon_data/AMAZON_FASHION_5.json' \n",
    "fashion_data = read_file(path, 2000)\n",
    "write_csv_file('multi_class_data/fashion.csv', fashion_data, 1000, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Music, label 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'amazon_Data/Digital_Music_5.json'\n",
    "music_data = read_file(path, 2000)\n",
    "write_csv_file('multi_class_data/music.csv', music_data, 1000, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sports, label 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'amazon_data/Sports_and_Outdoors_5.json'\n",
    "sport_data = read_file(path, 2000)\n",
    "write_csv_file('multi_class_data/sport.csv', sport_data, 1000, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pet Supplies, label 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'amazon_data/Pet_Supplies_5.json'\n",
    "pet_data = read_file(path, 2000)\n",
    "write_csv_file('multi_class_data/pet.csv', pet_data, 1000, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fashion = pd.read_csv('multi_class_data/fashion.csv')\n",
    "df_music = pd.read_csv('multi_class_data/music.csv')\n",
    "df_sport = pd.read_csv('multi_class_data/sport.csv')\n",
    "df_pet = pd.read_csv('multi_class_data/pet.csv')\n",
    "\n",
    "df = pd.concat([df_fashion, df_music, df_sport, df_pet], ignore_index=True)\n",
    "df = df.sample(frac=1, random_state = 42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the best sneakers by far! I had never owned a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The quality of the items were good and they we...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>These shoes are extremely comfortable, and fit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They worked extremely well, this is the only p...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Convenient packaging and reasonable pricing. N...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  label\n",
       "0  the best sneakers by far! I had never owned a ...      0\n",
       "1  The quality of the items were good and they we...      3\n",
       "2  These shoes are extremely comfortable, and fit...      0\n",
       "3  They worked extremely well, this is the only p...      3\n",
       "4  Convenient packaging and reasonable pricing. N...      2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained word embedding model\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biagram_depending_on_link(review, label):\n",
    "    \n",
    "    weights_each_type = {'ADJ': 3, 'ADV': 2, 'NOUN': 1, 'VERB': 4, 'ADP': 1, 'DET': 1, 'NUM': 1, 'PUNCT': 1, 'PRON': 1, 'PROPN': 1, 'SCONJ': 1, 'SYM': 1, 'X': 1, 'PART': 1, 'CCONJ': 1, 'INTJ': 1, 'AUX': 1, 'SPACE': 1, '': 1}\n",
    "\n",
    "    ## sentences preprocessing\n",
    "    doc = nlp(review)\n",
    "    sentences = [sent for sent in doc.sents]\n",
    "    sentences = [{token.text.lower() : (token.pos_, token.dep_) for token in sent if not token.is_stop and token.is_alpha} for sent in sentences]\n",
    "\n",
    "    ## get the biagrams\n",
    "    biagrams = []\n",
    "    for sent in sentences:\n",
    "        for i in range(len(sent)-1):\n",
    "            biagrams.append((list(sent.keys())[i], list(sent.keys())[i+1]))\n",
    "\n",
    "    \n",
    "    ### concatenate all the sentences in one\n",
    "    sent = {}\n",
    "    for sentence in sentences:\n",
    "        sent.update(sentence)\n",
    "\n",
    "    ### dico of how many times a biagram appears in the review\n",
    "    dico_biagrams = {}\n",
    "    for biagram in biagrams:\n",
    "        if biagram not in dico_biagrams and (biagram[1], biagram[0]) not in dico_biagrams:\n",
    "            dico_biagrams[biagram] = weights_each_type[sent[biagram[0]][0]] + weights_each_type[sent[biagram[1]][0]]\n",
    "        elif biagram in dico_biagrams:\n",
    "            dico_biagrams[biagram] +=  weights_each_type[sent[biagram[0]][0]] + weights_each_type[sent[biagram[1]][0]]\n",
    "        elif (biagram[1], biagram[0]) in dico_biagrams:\n",
    "            dico_biagrams[(biagram[1], biagram[0])] += weights_each_type[sent[biagram[0]][0]] + weights_each_type[sent[biagram[1]][0]]\n",
    "        \n",
    "\n",
    "    list_of_words = [word for sent in sentences for word in sent]\n",
    "    list_of_words = list(set(list_of_words))\n",
    "    ## create graph \n",
    "    G = nx.Graph()\n",
    "    ## nodes as words \n",
    "    G.add_nodes_from(list_of_words)\n",
    "\n",
    "    ## add edges\n",
    "    for biagram in dico_biagrams.keys():\n",
    "        G.add_edge(biagram[0], biagram[1], weight = dico_biagrams[biagram])\n",
    "\n",
    "    \n",
    "    # Get the node features\n",
    "    node_features = []\n",
    "    for node in G.nodes():\n",
    "        node_features.append(nlp.vocab[node].vector)\n",
    "    node_features = np.array(node_features)\n",
    "    # Get the edges\n",
    "    edges = []\n",
    "    for edge in G.edges():\n",
    "        edges.append([list(G.nodes()).index(edge[0]), list(G.nodes()).index(edge[1])])\n",
    "    edges = np.array(edges)\n",
    "    ## edge_attr \n",
    "    edges_attr  = []\n",
    "    for edge in G.edges():\n",
    "        edges_attr.append([G.edges[edge]['weight']])\n",
    "    edges_attr = np.array(edges_attr)\n",
    "    \n",
    "    # Get the label\n",
    "    label_value = int(label)\n",
    "\n",
    "\n",
    "    # Create a PyTorch Geometric Data object\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edges.T, dtype=torch.long)\n",
    "    y = torch.tensor(label_value, dtype=torch.float)\n",
    "    edge_attr = torch.tensor(edges_attr, dtype=torch.float)\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr = edge_attr, y=y)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_reviews = [] \n",
    "\n",
    "for i in range(800):\n",
    "    data = biagram_depending_on_link(df['reviewText'][i], df['label'][i])\n",
    "    list_of_reviews.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[11, 300], edge_index=[2, 9], edge_attr=[9, 1], y=0.0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(list_of_reviews, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "263\n",
      "tensor(3.)\n",
      "torch.Size([7, 300])\n",
      "639\n"
     ]
    }
   ],
   "source": [
    "for i,review in enumerate(train_data):\n",
    "    if review.edge_index.shape < torch.Size([2]):\n",
    "        print(review.edge_index.shape)\n",
    "        print(i)\n",
    "        print(review.y)\n",
    "        print(review.x.shape)\n",
    "        train_data.pop(i)\n",
    "print(len(train_data))\n",
    "\n",
    "for i,review in enumerate(test_data):\n",
    "    if review.edge_index.shape < torch.Size([2]):\n",
    "        print(review.edge_index.shape)\n",
    "        test_data.pop(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### is the data balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fashion reviews: 160\n",
      "Number of music reviews: 175\n",
      "Number of sport reviews: 160\n",
      "Number of pet reviews: 144\n"
     ]
    }
   ],
   "source": [
    "def get_numbers(data):\n",
    "    labels = [d.y.item() for d in data]\n",
    "    fashion = labels.count(0)   \n",
    "    music = labels.count(1)\n",
    "    sport = labels.count(2)\n",
    "    pet = labels.count(3)\n",
    "\n",
    "    return fashion, music, sport, pet\n",
    "\n",
    "fashion, music, sport, pet = get_numbers(train_data)\n",
    "\n",
    "print(f'Number of fashion reviews: {fashion}')\n",
    "print(f'Number of music reviews: {music}')\n",
    "print(f'Number of sport reviews: {sport}')\n",
    "print(f'Number of pet reviews: {pet}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 1.2134, Train Acc: 0.6041 Test Acc: 0.6000\n",
      "Epoch: 001, Loss: 0.5845, Train Acc: 0.8811 Test Acc: 0.8562\n",
      "Epoch: 002, Loss: 0.2824, Train Acc: 0.9452 Test Acc: 0.9000\n",
      "Epoch: 003, Loss: 0.1652, Train Acc: 0.9609 Test Acc: 0.9125\n",
      "Epoch: 004, Loss: 0.1215, Train Acc: 0.9875 Test Acc: 0.8938\n",
      "Epoch: 005, Loss: 0.0816, Train Acc: 0.9969 Test Acc: 0.9187\n",
      "Epoch: 006, Loss: 0.0586, Train Acc: 0.9953 Test Acc: 0.9062\n",
      "Epoch: 007, Loss: 0.0455, Train Acc: 0.9937 Test Acc: 0.9125\n",
      "Epoch: 008, Loss: 0.0601, Train Acc: 0.9906 Test Acc: 0.9000\n",
      "Epoch: 009, Loss: 0.0147, Train Acc: 1.0000 Test Acc: 0.8938\n",
      "\n",
      "GraphSage test accuracy: 89.38%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, global_add_pool, global_mean_pool  \n",
    "\n",
    "'''\n",
    "Graph SAGE: SAmpling and aggreGatE, \n",
    "Samples only a subset of neighboring nodes at different depth layers, \n",
    "and then the aggregator takes neighbors of the previous layers and aggregates them\n",
    "'''\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "  \"\"\"GraphSAGE\"\"\"\n",
    "  def __init__(self, num_node_features, hidden_dim, num_classes):\n",
    "    super().__init__()\n",
    "    self.sage1 = SAGEConv(num_node_features, hidden_dim*2)\n",
    "    self.sage2 = SAGEConv(hidden_dim*2, hidden_dim)\n",
    "    self.sage3 = SAGEConv(hidden_dim, hidden_dim)\n",
    "    self.sage4 = SAGEConv(hidden_dim, num_classes)\n",
    "    self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                      lr=0.0001,\n",
    "                                        weight_decay=5e-4)\n",
    "                                      \n",
    "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    ## layer 1 \n",
    "    h = self.sage1(x, edge_index)\n",
    "    h = torch.relu(h)\n",
    "    h = F.dropout(h, p=0.2, training=self.training)\n",
    "\n",
    "    ## layer 2\n",
    "\n",
    "    h = self.sage2(h, edge_index)\n",
    "    h = torch.relu(h)\n",
    "    h = F.dropout(h, p=0.2, training=self.training)\n",
    "\n",
    "    # layer 3 \n",
    "    h = self.sage3(h, edge_index)\n",
    "    h = torch.relu(h)\n",
    "    h = F.dropout(h, p=0.5, training=self.training)\n",
    "\n",
    "     # layer 4\n",
    "    h = self.sage3(h, edge_index)\n",
    "    h = torch.relu(h)\n",
    "    h = F.dropout(h, p=0.5, training=self.training)\n",
    "\n",
    "     # layer 5\n",
    "    h = self.sage3(h, edge_index)\n",
    "    h = torch.relu(h)\n",
    "    h = F.dropout(h, p=0.2, training=self.training)\n",
    "\n",
    "    ## layer 6\n",
    "    h = self.sage4(h, edge_index)\n",
    "    h = global_mean_pool(h, torch.zeros(h.size(0), dtype=torch.long).to(self.device))\n",
    "    return h, F.log_softmax(h, dim=1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "def train(model, loader, device):\n",
    "    model.train()\n",
    "    optimizer = model.optimizer\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        _, out = model(data.x.float(), data.edge_index)\n",
    "        loss = F.nll_loss(out, data.y.long())\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return total_loss / len(loader.dataset)\n",
    "    \n",
    "    \n",
    "# Define the testing loop\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        _, out = model(data.x.float(), data.edge_index)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += pred.eq(data.y.long()).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "\n",
    "# Define the model and optimizer\n",
    "model = GraphSAGE(300, 300, 4).to(device)\n",
    "\n",
    "# Train the model\n",
    "train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "for epoch in range(0, 10):\n",
    "    loss = train(model, train_loader, device)\n",
    "    train_acc = test(model, train_loader, device)\n",
    "    test_acc = test(model, test_loader, device)\n",
    "    print(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}\", f\"Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "\n",
    "print(f'\\nGraphSage test accuracy: {test(model, test_loader, device)*100:.2f}%\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
